<img src="https://cdn.prod.website-files.com/677c400686e724409a5a7409/6790ad949cf622dc8dcd9fe4_nextwork-logo-leather.svg" alt="NextWork" width="300" />

# Automate Testing with GitHub Actions

**Project Link:** [View Project](http://learn.nextwork.org/projects/ai-devops-githubactions)

**Author:** Roy Piring Jr  
**Email:** rpiringhawaii@gmail.com

---

![Image](http://learn.nextwork.org/refreshed_maroon_timid_jujube/uploads/ai-devops-githubactions_i1j2k3l4)

---

## Introducing Today's Project!

This project implements a CI/CD pipeline using GitHub Actions to automate semantic testing for a Retrieval-Augmented Generation (RAG) application. The objective is to ensure that changes to code or data do not silently degrade application behavior.

The focus is on applying production-grade testing practices to AI systems, where output variability introduces unique challenges. This project demonstrates how deterministic testing can be enforced in AI-driven applications through controlled execution modes.

### Key services and concepts

The primary services used in this project are GitHub Actions and Git.

GitHub Actions provides the automation framework for running tests on every code change. Git enables version control and serves as the trigger mechanism for CI workflows.

Key concepts explored include CI/CD automation, semantic testing, deterministic testing strategies for AI systems, FastAPI-based services, and RAG application behavior validation.

### Challenges and wins

This project required approximately three hours to complete. The most challenging aspect was understanding how semantic testing behaves in the presence of non-deterministic AI outputs, particularly when validating responses generated by an LLM.

The key success was achieving a reliable CI pipeline that automatically runs semantic tests and fails predictably when data quality or application behavior degrades. This validated that AI systems can be tested rigorously when designed with determinism in mind.

### Why I did this project

I completed this project to understand how CI/CD practices apply to AI-driven applications, where traditional unit testing is insufficient.

The most important takeaway is the use of mock LLM modes to enforce deterministic behavior during testing. This approach enables reliable automation and protects production systems from silent regressions caused by changes in data or prompts.

---

## Setting Up Your RAG API

The RAG API is set up by cloning the repository, configuring the environment, and installing required dependencies.

A RAG API indexes documents into a vector database and retrieves relevant context in response to queries. This stable baseline is required for CI/CD because automated tests depend on predictable application behavior. Establishing a known-good local state ensures that failures observed in CI are meaningful and actionable.

### Local API verification

The API was verified locally by submitting a query related to the project documentation. The response accurately summarized the project's goals and key concepts.

This confirms that the retrieval and response pipeline is functioning correctly before introducing automated testing.

![Image](http://learn.nextwork.org/refreshed_maroon_timid_jujube/uploads/ai-devops-githubactions_i9j0k1l2)

---

## Initializing Git and Pushing to GitHub

Git is initialized to enable version control and track changes over time. Version control is foundational for CI/CD because automated workflows are triggered by commits and pushes.

Tracking changes ensures that every modification can be tested, reviewed, and reverted if necessary.

### Git initialization and first commit

Git was initialized using git init, creating a local repository. Files were staged with git add . and committed using git commit -m "Initial commit".

A .gitignore file was included to prevent unnecessary or sensitive files from being tracked. This keeps the repository clean and reduces noise in CI workflows.

### Pushing to GitHub for CI/CD

Pushing the repository to GitHub makes the code available for automated workflows. Each push acts as a trigger for CI processes, enabling automated testing and immediate feedback.

This ensures that every change is evaluated consistently before being merged or deployed.

![Image](http://learn.nextwork.org/refreshed_maroon_timid_jujube/uploads/ai-devops-githubactions_y5z6a7b8)

---

## Creating Semantic Tests

Semantic tests validate the overall behavior of the RAG API rather than individual functions. These tests ensure the system retrieves relevant context and produces meaningful responses.

Unlike unit tests, semantic tests evaluate correctness from a user perspective, making them essential for AI-driven systems where output quality matters more than individual code paths.

### Non-deterministic output observation

Repeated execution of the same query produced different responses. This variability is expected with LLMs but problematic for automated testing.

Non-deterministic outputs prevent reliable test assertions, making CI pipelines unstable and untrustworthy. This observation highlighted the need for deterministic testing controls.

---

## Adding Mock LLM Mode

Mock LLM mode is introduced to simulate LLM behavior without external API calls. Instead of generating responses dynamically, the system returns consistent, predefined outputs.

This eliminates variability and allows semantic tests to be deterministic and repeatable. The mock mode is tested locally to confirm consistent behavior before being used in CI.

The implementation is committed and pushed to ensure CI runs against the deterministic configuration.

How mock mode solves the problem

Mock mode removes randomness from the response generation step. By returning known outputs, tests can reliably compare actual results against expected results.

This transforms semantic testing from probabilistic validation into deterministic verification.

### How mock mode solves the problem

### Mock LLM mode for CI testing

In CI environments, mock LLM mode ensures that tests are fast, predictable, and cost-free. Without mock mode, tests would be flaky due to LLM variability and external dependencies.

Deterministic behavior is a requirement for reliable CI pipelines, making mock mode essential for testing AI systems at scale.

---

## Creating GitHub Actions Workflow

A GitHub Actions workflow is created to automatically run semantic tests whenever code is pushed to the repository.

The workflow executes predefined queries against the RAG API and validates responses against expected outputs. Failures indicate regressions in code, data, or configuration.

### Workflow automation and CI testing

The workflow file, main.yml, is created and committed to the repository. Once pushed, GitHub automatically runs the workflow on every change to the main branch.

This provides immediate feedback on application health and prevents broken changes from progressing unnoticed.

---

## Testing Data Quality

In this step, the CI workflow is intentionally triggered with flawed data to validate that automated semantic testing correctly detects semantic regressions. This ensures the CI pipeline protects the application not only from broken code, but also from degraded knowledge quality.

By committing and pushing incorrect or incomplete data, the workflow is expected to fail. This demonstrates that semantic validation is functioning as designed and that the RAG system is guarded against silent quality regressions.

### Data quality and CI protection

The missing keyword was "orchestration". The semantic test failed because the RAG API returned an irrelevant response due to the flawed data. Without CI, this degraded content would have been served to users, providing inaccurate information.

![Image](http://learn.nextwork.org/refreshed_maroon_timid_jujube/uploads/ai-devops-githubactions_i1j2k3l4)

---

## Testing Another Data Quality Issue

### Data quality and CI protection

---

## Scaling with Multiple Documents

In this step, the project is restructured to support increased complexity and long-term maintainability of the RAG application. The updated folder layout enables modular development, clearer separation of concerns, and easier extension of the knowledge base.

This approach scales effectively by promoting reuse, reducing coupling between components, and supporting collaborative development without destabilizing existing functionality.

### Docs folder structure and CI scaling

The docs folder organizes content by topic and domain, creating a structured and navigable knowledge base for the RAG API. The embed_docs.py script indexes and embeds all documents in this folder, converting them into a format suitable for vector storage and semantic retrieval.

CI validated the full document set and detected no regressions, confirming that the RAG API continued to retrieve accurate information after structural changes. This design supports growth by allowing new documents to be added without disrupting existing retrieval behavior or CI guarantees.

![Image](http://learn.nextwork.org/refreshed_maroon_timid_jujube/uploads/ai-devops-githubactions_g5h6i7j8)

---

---
